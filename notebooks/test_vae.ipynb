{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57fa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from src.model.VAELightningModule import VAELightningModule\n",
    "from src.model.AudioVAE import AudioVAE\n",
    "from src.loss_fn.VAELossCalculator import VAELossCalculator\n",
    "from src.loss_fn.WaveformLoss import WaveformLoss\n",
    "from tqdm import tqdm\n",
    "import musdb\n",
    "\n",
    "# Import your module class\n",
    "# from your_file import VAELightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dfffacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = musdb.DB(root=\"/data1/midi_generation_datasets/musdb/musdb18\", subsets=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "231f761f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[209.906009,\n",
       " 200.327007,\n",
       " 179.393991,\n",
       " 219.192993,\n",
       " 336.84,\n",
       " 219.925011,\n",
       " 254.710998,\n",
       " 221.05,\n",
       " 275.626009,\n",
       " 250.019002,\n",
       " 253.933991,\n",
       " 190.281995,\n",
       " 186.796009,\n",
       " 273.385011,\n",
       " 430.175011,\n",
       " 317.080998,\n",
       " 141.473991,\n",
       " 205.272993,\n",
       " 267.451995,\n",
       " 281.411995,\n",
       " 292.621995,\n",
       " 227.229002,\n",
       " 269.8,\n",
       " 198.657007,\n",
       " 246.522993,\n",
       " 271.965011,\n",
       " 343.513991,\n",
       " 162.503991,\n",
       " 75.996009,\n",
       " 221.398005,\n",
       " 212.046009,\n",
       " 317.68,\n",
       " 286.646009,\n",
       " 250.896009,\n",
       " 243.482993,\n",
       " 177.198005,\n",
       " 395.462993,\n",
       " 252.96,\n",
       " 312.467007,\n",
       " 331.241995,\n",
       " 234.483991,\n",
       " 246.790998,\n",
       " 305.520998,\n",
       " 244.26,\n",
       " 320.097007,\n",
       " 252.849002,\n",
       " 175.652993,\n",
       " 234.91,\n",
       " 207.698005,\n",
       " 234.183991]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durations = [track.duration for track in ds]\n",
    "durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba859de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8118f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0cfc918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.327007"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1347f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = track.audio.T\n",
    "sr = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddb473c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track.rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c61b157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/anaconda3/envs/audio_separation/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"../checkpoints/big_model/last.ckpt\", map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "# Remove the 'model.' prefix added by the LightningModule\n",
    "new_state_dict = {\n",
    "    k.replace(\"model.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"model.\")\n",
    "}\n",
    "\n",
    "model = AudioVAE(base_channels=128, strides=[2, 4, 4, 8])\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c06a8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_musdb_track(\n",
    "    model,\n",
    "    track,\n",
    "    track_name: str,\n",
    "    chunk_size=int(1.5 * 44100),\n",
    "    sample_rate: int = 44100,\n",
    "    overlap=1024,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"reconstructions\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a musdb Track, reconstructs it in chunks, and saves the result.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Setup Model\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Get Audio (MUSDB yields [samples, channels], we need [channels, samples])\n",
    "    # track.audio is the stereo mixture\n",
    "    original_audio = torch.from_numpy(track).float()\n",
    "    num_channels, total_samples = original_audio.shape\n",
    "\n",
    "    # 3. Initialize reconstruction buffer\n",
    "    reconstructed_full = torch.zeros_like(original_audio)\n",
    "    # Weight buffer for linear cross-fading (optional, here we use simple overwrite)\n",
    "\n",
    "    # 4. Process in Chunks\n",
    "    step = chunk_size - overlap\n",
    "    print(f\"Reconstructing track: {track_name}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in tqdm(range(0, total_samples, step)):\n",
    "            end = min(start + chunk_size, total_samples)\n",
    "\n",
    "            # Extract chunk and pad if it's the last short chunk\n",
    "            chunk = original_audio[:, start:end]\n",
    "            actual_len = chunk.shape[1]\n",
    "\n",
    "            if actual_len < chunk_size:\n",
    "                chunk = torch.nn.functional.pad(chunk, (0, chunk_size - actual_len))\n",
    "\n",
    "            # Prepare for VAE (Batch, Channels, Length)\n",
    "            input_tensor = chunk.unsqueeze(0).to(device)\n",
    "\n",
    "            # VAE Forward Pass\n",
    "            # Based on your module: recon, z, mean, logvar = self.forward(x)\n",
    "            recon, _, _, _ = model(input_tensor)\n",
    "\n",
    "            # Remove batch dim and crop padding if necessary\n",
    "            recon_chunk = recon.squeeze(0).cpu()\n",
    "            recon_chunk = recon_chunk[:, :actual_len]\n",
    "\n",
    "            # Insert into full buffer\n",
    "            # Simple stitching:\n",
    "            reconstructed_full[:, start:end] = recon_chunk\n",
    "\n",
    "    # 5. Save Files\n",
    "    orig_path = f\"{output_dir}/{track_name.replace(' ', '_')}_original.wav\"\n",
    "    recon_path = f\"{output_dir}/{track_name.replace(' ', '_')}_reconstructed.wav\"\n",
    "\n",
    "    torchaudio.save(orig_path, original_audio, sample_rate=sample_rate)\n",
    "    torchaudio.save(recon_path, reconstructed_full, sample_rate=sample_rate)\n",
    "\n",
    "    print(f\"Saved: {recon_path}\")\n",
    "    return reconstructed_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "923ccc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing track: Al James - Schoolboy Facination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 15/136 [00:00<00:04, 27.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [00:04<00:00, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reconstructions/Al_James_-_Schoolboy_Facination_reconstructed.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0033,  0.0070,  0.0169,  ...,  0.0200,  0.0143,  0.0191],\n",
       "        [ 0.0160,  0.0129,  0.0215,  ...,  0.0335,  0.0346,  0.0345]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_musdb_track(model=model, track=x, track_name=track.name, device=\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1119b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_separation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
