{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e57fa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import torchaudio\n",
    "from src.model.VAELightningModule import VAELightningModule\n",
    "from src.model.AudioVAE import AudioVAE\n",
    "from src.loss_fn.VAELossCalculator import VAELossCalculator\n",
    "from src.loss_fn.WaveformLoss import WaveformLoss\n",
    "from tqdm import tqdm\n",
    "from src.dataset.GTZANDataset import GTZANDataset\n",
    "\n",
    "# Import your module class\n",
    "# from your_file import VAELightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba859de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GTZANDataset(root=\"/data1/midi_generation_datasets/gtzan\", split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5550d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blues.00081.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 661794])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(ds.audio_files))\n",
    "track_name = Path(ds.audio_files[idx]).name\n",
    "print(track_name)\n",
    "audio, sr = torchaudio.load(ds.audio_files[idx])\n",
    "audio = torch.cat([audio, audio], dim=0)\n",
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61b157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/anaconda3/envs/audio_separation/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\n",
    "    \"../checkpoints/audio_vae_with_warmup/last.ckpt\", map_location=\"cpu\"\n",
    ")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "# Remove the 'model.' prefix added by the LightningModule\n",
    "new_state_dict = {\n",
    "    k.replace(\"model.\", \"\"): v for k, v in state_dict.items() if k.startswith(\"model.\")\n",
    "}\n",
    "\n",
    "model = AudioVAE(\n",
    "    base_channels=64,\n",
    "    strides=[2, 4, 4, 4, 4],\n",
    "    channel_mults=[1, 2, 4, 8, 8],\n",
    "    latent_dim=128,\n",
    ")\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c06a8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_musdb_track(\n",
    "    model,\n",
    "    track,\n",
    "    track_name: str,\n",
    "    chunk_size=int(1.5 * 22050),\n",
    "    sample_rate: int = 22050,\n",
    "    overlap=1024,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"reconstructions\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a musdb Track, reconstructs it in chunks, and saves the result.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Setup Model\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Get Audio (MUSDB yields [samples, channels], we need [channels, samples])\n",
    "    # track.audio is the stereo mixture\n",
    "    if not isinstance(track, torch.Tensor):\n",
    "        original_audio = torch.from_numpy(track).float()\n",
    "    else:\n",
    "        original_audio = track.float()\n",
    "    num_channels, total_samples = original_audio.shape\n",
    "\n",
    "    # 3. Initialize reconstruction buffer\n",
    "    reconstructed_full = torch.zeros_like(original_audio)\n",
    "    # Weight buffer for linear cross-fading (optional, here we use simple overwrite)\n",
    "\n",
    "    # 4. Process in Chunks\n",
    "    step = chunk_size - overlap\n",
    "    print(f\"Reconstructing track: {track_name}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in tqdm(range(0, total_samples, step)):\n",
    "            end = min(start + chunk_size, total_samples)\n",
    "\n",
    "            # Extract chunk and pad if it's the last short chunk\n",
    "            chunk = original_audio[:, start:end]\n",
    "            actual_len = chunk.shape[1]\n",
    "\n",
    "            if actual_len < chunk_size:\n",
    "                chunk = torch.nn.functional.pad(chunk, (0, chunk_size - actual_len))\n",
    "\n",
    "            # Prepare for VAE (Batch, Channels, Length)\n",
    "            input_tensor = chunk.unsqueeze(0).to(device)\n",
    "\n",
    "            # VAE Forward Pass\n",
    "            # Based on your module: recon, z, mean, logvar = self.forward(x)\n",
    "            recon, _, _, _ = model(input_tensor)\n",
    "\n",
    "            # Remove batch dim and crop padding if necessary\n",
    "            recon_chunk = recon.squeeze(0).cpu()\n",
    "            recon_chunk = recon_chunk[:, :actual_len]\n",
    "\n",
    "            # Insert into full buffer\n",
    "            # Simple stitching:\n",
    "            reconstructed_full[:, start:end] = recon_chunk\n",
    "\n",
    "    # 5. Save Files\n",
    "    orig_path = f\"{output_dir}/{track_name.replace(' ', '_')}_original.wav\"\n",
    "    recon_path = f\"{output_dir}/{track_name.replace(' ', '_')}_reconstructed.wav\"\n",
    "\n",
    "    torchaudio.save(orig_path, original_audio, sample_rate=sample_rate)\n",
    "    torchaudio.save(recon_path, reconstructed_full, sample_rate=sample_rate)\n",
    "\n",
    "    print(f\"Saved: {recon_path}\")\n",
    "    return reconstructed_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "923ccc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing track: blues.00081.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 42.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reconstructions/blues.00081.wav_reconstructed.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1885, 0.2607, 0.2502,  ..., 0.0069, 0.0073, 0.0123],\n",
       "        [0.1878, 0.2601, 0.2496,  ..., 0.0061, 0.0065, 0.0114]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_musdb_track(\n",
    "    model=model, track=audio, track_name=track_name, device=\"cuda:3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1119b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_separation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
